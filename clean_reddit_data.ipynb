{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "flying-beauty",
   "metadata": {},
   "source": [
    "Runs import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "representative-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-swedish",
   "metadata": {},
   "source": [
    "Finds files in the data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "earned-soundtrack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eddychiao/Documents/Georgia Tech/CS/GaTech-CS/MSCS/CS 6474/project/april-october\n",
      "['title', 'comment']\n",
      "['cleaned_titles.zip', '.DS_Store']\n"
     ]
    }
   ],
   "source": [
    "data_directory = os.getcwd() + '/april-october'\n",
    "\n",
    "cwd, files, filenames = next(os.walk(data_directory))\n",
    "print(cwd)\n",
    "print(files)\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-wireless",
   "metadata": {},
   "source": [
    "# Cleaning Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-bennett",
   "metadata": {},
   "source": [
    "Reads data in specific csv (we'll just look at title.csv and comment.csv for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "reverse-austin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title_nov_2020.csv', 'title_Jan_2021.csv', '.DS_Store', 'title_Dec_2020.csv', 'title_Aug_2020.csv', 'title_Oct_2020.csv', 'title_Jun_2020.csv', 'title_April_2020.csv', 'title_Feb_2021.csv', 'title_Mar_2021.csv', 'title_Jul_2020.csv', 'title_Sep_2020.csv', 'title_May_2020.csv']\n"
     ]
    }
   ],
   "source": [
    "title_cwd = cwd + '/title'\n",
    "_, _, title_names = next(os.walk(title_cwd))\n",
    "print(title_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "irish-occupation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title_nov_2020.csv', 'title_Jan_2021.csv', 'title_Dec_2020.csv', 'title_Aug_2020.csv', 'title_Oct_2020.csv', 'title_Jun_2020.csv', 'title_April_2020.csv', 'title_Feb_2021.csv', 'title_Mar_2021.csv', 'title_Jul_2020.csv', 'title_Sep_2020.csv', 'title_May_2020.csv']\n"
     ]
    }
   ],
   "source": [
    "title_names = [file for file in title_names if file[-4:] == '.csv']\n",
    "print(title_names)\n",
    "\n",
    "# all_csvs = []title_names\n",
    "# # reads all the csvs in the data folder\n",
    "# for file in filenames:\n",
    "#     all_csvs.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "received-environment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title        timestamp\n",
      "0                            J POW'S PRINTER AWAITS!  2020-10-31 4:02\n",
      "1                                            Yall rn  2020-10-31 4:02\n",
      "2                                          TSLA PUTS  2020-10-31 4:03\n",
      "3  TSLA Stonks Dipped Just To Scare You For Hallo...  2020-10-31 4:04\n",
      "4                   My autistic ass after this week.  2020-10-31 4:11\n",
      "(24644, 2)\n",
      "\n",
      "                                               Title        timestamp\n",
      "0       How my last day of trading in 2020 felt like  2021-01-01 3:59\n",
      "1                          Me investing in anything.  2021-01-01 3:56\n",
      "2                                The next yolo play?  2021-01-01 3:53\n",
      "3  A visual on how my last day trading in 2020 fe...  2021-01-01 3:48\n",
      "4  Got this succulent Chinese meal with this fort...  2021-01-01 3:47\n",
      "(54279, 2)\n",
      "\n",
      "                                               Title        timestamp\n",
      "0               üöÄüöÄüöÄüñïüè≥Ô∏è‚Äçüåàüêª pltr 12/4 40c avg cost .34  2020-12-01 3:56\n",
      "1  My Tesla Calls at close - Gain/Loss porn in on...  2020-12-01 3:54\n",
      "2   $Nga lion electric next generation tesla gogo!!!  2020-12-01 3:51\n",
      "3         Robinhood always lookin out for us retards  2020-12-01 3:51\n",
      "4      Just a friendly reminder that never lose hope  2020-12-01 3:50\n",
      "(31788, 2)\n",
      "\n",
      "                                               Title    timestamp\n",
      "0  [Repost] J. Powell - Late night at the stock m...  8/2/20 3:59\n",
      "1  BYTEDANCE agrees to sell TikTok to a US based ...  8/2/20 3:53\n",
      "2              A Three for One Tendie Stock Split DD  8/2/20 3:48\n",
      "3                 A Three for One Tendie Stock Split  8/2/20 3:46\n",
      "4                             Tim AAPL Yolo 18k gain  8/2/20 3:42\n",
      "(20541, 2)\n",
      "\n",
      "                                               Title     timestamp\n",
      "0  TSLA‚Äôs - 3rd party stock screeners confused af...  10/2/20 3:58\n",
      "1  Best strategies to make income if you have 1 m...  10/2/20 3:58\n",
      "2  Orange man in quarantine since top aide down w...  10/2/20 3:58\n",
      "3  Help me get first (or last) on my school proje...  10/2/20 3:51\n",
      "4                                              RDS.B  10/2/20 3:46\n",
      "(15368, 2)\n",
      "\n",
      "                                               Title    timestamp\n",
      "0  Someone educate me. How is this possible and w...  6/2/20 3:48\n",
      "1  Anyone know why LULU keeps going up and up ? E...  6/2/20 3:48\n",
      "2                        Consistent ALPHA in May üò§üò§üò§  6/2/20 3:45\n",
      "3  Is anyone looking at SWBI? They have some pote...  6/2/20 3:43\n",
      "4         Kevin o Leary investing in a penny stock ?  6/2/20 3:40\n",
      "(22514, 2)\n",
      "\n",
      "                                               Title    timestamp\n",
      "0                                   My SPY play idea  4/2/20 3:56\n",
      "1                               $MO Puts? $WLL Puts?  4/2/20 3:55\n",
      "2  No one‚Äôs talking that fucking APT made +100% i...  4/2/20 3:50\n",
      "3                        End of bearmarket confirmed  4/2/20 3:48\n",
      "4                        Is this one of you retards?  4/2/20 3:43\n",
      "(25334, 2)\n",
      "\n",
      "                                       Title            timestamp\n",
      "0                            Since of pride.  2021-02-28 03:59:16\n",
      "1                              Communication  2021-02-28 03:58:35\n",
      "2  Retail investors manipulating the market?  2021-02-28 03:58:21\n",
      "3                 üôåüèºüíéAPES TOGETHER STRONGüíéüôåüèº  2021-02-28 03:57:49\n",
      "4                      Moonday is coming !!!  2021-02-28 03:56:55\n",
      "(320000, 2)\n",
      "\n",
      "                                               Title        timestamp\n",
      "0            FHA housing mortgage delinquency rates.  2021-03-01 0:02\n",
      "1                             GME Shares, LEAPS, FDs  2021-03-01 0:13\n",
      "2    BABA ANT DD -China maybe first to go cashless ü•∏  2021-03-01 0:20\n",
      "3                                    AMC \"BULL TRAP\"  2021-03-01 0:56\n",
      "4  Monday is coming, stimmy is coming, GME is coming  2021-03-01 0:59\n",
      "(91630, 2)\n",
      "\n",
      "                                               Title    timestamp\n",
      "0                     Schrodinger (SDGR) - DD Part 2  7/2/20 3:59\n",
      "1  As one concerned trader to another in this tim...  7/2/20 3:57\n",
      "2                       Little trip down memory lane  7/2/20 3:53\n",
      "3          Greetings Young Lads, Warren Buffett Here  7/2/20 3:51\n",
      "4  Six figures of gains cashed out to where I can...  7/2/20 3:50\n",
      "(18905, 2)\n",
      "\n",
      "                                               Title    timestamp\n",
      "0  I see a lot of options here but does anyone ho...  9/2/20 3:59\n",
      "1    Thanks WSB, my sole source of investment advice  9/2/20 3:59\n",
      "2                                    Diamond hands üíé  9/2/20 3:57\n",
      "3   When do you guys think the market will sell off?  9/2/20 3:50\n",
      "4  TSLA and AAPL. $609 -&gt; $3.4k ($4k EOW by cr...  9/2/20 3:50\n",
      "(21122, 2)\n",
      "\n",
      "                                               Title    timestamp\n",
      "0                              1st Thing: Bring lube  5/2/20 3:56\n",
      "1  The Bond Market Crash Begins on May 12th and 13th  5/2/20 3:55\n",
      "2                   Make Insider Trading Legal Again  5/2/20 3:52\n",
      "3                                           RETREAT!  5/2/20 3:50\n",
      "4                                     u/BryGuySaysHi  5/2/20 3:47\n",
      "(18693, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Shows preview of each csv\n",
    "for csv in title_names:\n",
    "    csv_title = pd.read_csv(title_cwd + '/' + csv)\n",
    "    csv_title = csv_title.dropna()\n",
    "    print(csv_title.head(5))\n",
    "    print(csv_title.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "burning-routine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title_nov_2020.csv\n",
      "                                               Title  timestamp\n",
      "0                               j pow printer awaits 2020-10-31\n",
      "1                                            yall rn 2020-10-31\n",
      "2                                          tsla puts 2020-10-31\n",
      "3  tsla stonks dipped just to scare you for hallo... 2020-10-31\n",
      "4                    my autistic ass after this week 2020-10-31\n",
      "\n",
      "title_Jan_2021.csv\n",
      "                                               Title  timestamp\n",
      "0            how my last day of trading in felt like 2021-01-01\n",
      "1                           me investing in anything 2021-01-01\n",
      "2                                 the next yolo play 2021-01-01\n",
      "3   a visual on how my last day trading in felt like 2021-01-01\n",
      "4  got this succulent chinese meal with this fort... 2021-01-01\n",
      "\n",
      "title_Dec_2020.csv\n",
      "                                               Title  timestamp\n",
      "0                                     pltrc avg cost 2020-12-01\n",
      "1  my tesla calls at close gain loss porn in one ... 2020-12-01\n",
      "2       nga lion electric next generation tesla gogo 2020-12-01\n",
      "3         robinhood always lookin out for us retards 2020-12-01\n",
      "4        just friendly reminder that never lose hope 2020-12-01\n",
      "\n",
      "title_Aug_2020.csv\n",
      "                                               Title  timestamp\n",
      "0       repost powell late night at the stock market 2020-08-02\n",
      "1  bytedance agrees to sell tiktok to us based co... 2020-08-02\n",
      "2              a three for one tendie stock split dd 2020-08-02\n",
      "3                 a three for one tendie stock split 2020-08-02\n",
      "4                                tim aapl yolok gain 2020-08-02\n",
      "\n",
      "title_Oct_2020.csv\n",
      "                                               Title  timestamp\n",
      "0  tsla srd party stock screeners confused af abo... 2020-10-02\n",
      "1  best strategies to make income if you have mil... 2020-10-02\n",
      "2  orange man in quarantine since top aide down w... 2020-10-02\n",
      "3     help me get first or last on my school project 2020-10-02\n",
      "4                                              rds b 2020-10-02\n",
      "\n",
      "title_Jun_2020.csv\n",
      "                                               Title  timestamp\n",
      "0  someone educate me how is this possible and wh... 2020-06-02\n",
      "1  anyone know why lulu keeps going up and up eve... 2020-06-02\n",
      "2                            consistent alpha in may 2020-06-02\n",
      "3  is anyone looking at swbi they have some poten... 2020-06-02\n",
      "4               kevin leary investing in penny stock 2020-06-02\n",
      "\n",
      "title_April_2020.csv\n",
      "                                               Title  timestamp\n",
      "0                                   my spy play idea 2020-04-02\n",
      "1                                   mo puts wll puts 2020-04-02\n",
      "2  no one talking that fucking apt made in week p... 2020-04-02\n",
      "3                        end of bearmarket confirmed 2020-04-02\n",
      "4                         is this one of you retards 2020-04-02\n",
      "\n",
      "title_Feb_2021.csv\n",
      "                                      Title  timestamp\n",
      "0                            since of pride 2021-02-28\n",
      "1                             communication 2021-02-28\n",
      "2  retail investors manipulating the market 2021-02-28\n",
      "3                      apes together strong 2021-02-28\n",
      "4                         moonday is coming 2021-02-28\n",
      "\n",
      "title_Mar_2021.csv\n",
      "                                             Title  timestamp\n",
      "0           fha housing mortgage delinquency rates 2021-03-01\n",
      "1                             gme shares leaps fds 2021-03-01\n",
      "2     baba ant dd china maybe first to go cashless 2021-03-01\n",
      "3                                    amc bull trap 2021-03-01\n",
      "4  monday is coming stimmy is coming gme is coming 2021-03-01\n",
      "\n",
      "title_Jul_2020.csv\n",
      "                                               Title  timestamp\n",
      "0                           schrodinger sdgr dd part 2020-07-02\n",
      "1  as one concerned trader to another in this tim... 2020-07-02\n",
      "2                       little trip down memory lane 2020-07-02\n",
      "3           greetings young lads warren buffett here 2020-07-02\n",
      "4  six figures of gains cashed out to where can g... 2020-07-02\n",
      "\n",
      "title_Sep_2020.csv\n",
      "                                               Title  timestamp\n",
      "0  i see lot of options here but does anyone hold... 2020-09-02\n",
      "1     thanks wsb my sole source of investment advice 2020-09-02\n",
      "2                                      diamond hands 2020-09-02\n",
      "3    when do you guys think the market will sell off 2020-09-02\n",
      "4   tsla and aapl gtkk eow by credit spreads on tsla 2020-09-02\n",
      "\n",
      "title_May_2020.csv\n",
      "                                         Title  timestamp\n",
      "0                         1st thing bring lube 2020-05-02\n",
      "1  the bond market crash begins on mayth andth 2020-05-02\n",
      "2             make insider trading legal again 2020-05-02\n",
      "3                                      retreat 2020-05-02\n",
      "4                               u bryguysayshi 2020-05-02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for csv in title_names:\n",
    "    print(csv)\n",
    "    csv_title = pd.read_csv(title_cwd + '/' + csv)\n",
    "    csv_title = csv_title.dropna()\n",
    "    csv_title.Title = csv_title.Title.str.lower()\n",
    "    \n",
    "    title_data_cleaned = csv_title.copy()\n",
    "    \n",
    "    title_data_cleaned.Title = title_data_cleaned.Title.apply(lambda x:re.sub('@[^\\s]+','',x))\n",
    "    # Remove URLS\n",
    "    title_data_cleaned.Title = title_data_cleaned.Title.apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n",
    "    # Remove all the special characters\n",
    "    title_data_cleaned.Title = title_data_cleaned.Title.apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n",
    "    # Remove numbers\n",
    "    title_data_cleaned.Title = title_data_cleaned.Title.apply(lambda x:re.sub(' \\d+', '', x))\n",
    "    # Remove all single characters\n",
    "    title_data_cleaned.Title = title_data_cleaned.Title.apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x))\n",
    "    # Remove single characters again because still leaving some\n",
    "    title_data_cleaned.Title = title_data_cleaned.Title.apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x))\n",
    "    # Removes specific time of day from timestamp\n",
    "#     title_data_cleaned.timestamp = pd.to_datetime(title_data_cleaned.timestamp).dt.date\n",
    "    # CUTOFFS: 0:00, 9:00, 13:00, 17:00\n",
    "\n",
    "    title_data_cleaned.timestamp = pd.to_datetime(title_data_cleaned.timestamp).dt.floor('h')\n",
    "#     print(title_data_cleaned.timestamp)\n",
    "\n",
    "\n",
    "    title_data_cleaned = title_data_cleaned.loc[pd.to_datetime(title_data_cleaned.timestamp).dt.hour < 16]\n",
    "    title_data_cleaned.loc[pd.to_datetime(title_data_cleaned.timestamp).dt.time < datetime.time(9, 30, 0), 'time'] = datetime.time(0)\n",
    "    title_data_cleaned.loc[pd.to_datetime(title_data_cleaned.timestamp).dt.time >= datetime.time(9, 30, 0), 'time'] = datetime.time(9, 30, 0)\n",
    "    title_data_cleaned.loc[pd.to_datetime(title_data_cleaned.timestamp).dt.time >= datetime.time(12, 30, 0), 'time'] = datetime.time(12, 30, 0)\n",
    "\n",
    "    \n",
    "#     title_data_cleaned.loc[pd.to_datetime(title_data_cleaned.timestamp).dt.hour >= 16, 'time'] = datetime.time(17)\n",
    "#     print(type(title_data_cleaned.timestamp))\n",
    "\n",
    "    title_data_cleaned.timestamp = pd.to_datetime(title_data_cleaned.timestamp).dt.date\n",
    "    \n",
    "    title_data_cleaned.timestamp = pd.to_datetime(title_data_cleaned.timestamp).dt.strftime('%Y-%m-%d')\n",
    "    title_data_cleaned.time = title_data_cleaned.time.astype(str)\n",
    "    \n",
    "    \n",
    "    title_data_cleaned.timestamp = pd.to_datetime(title_data_cleaned.timestamp + ' ' + title_data_cleaned.time)\n",
    "    title_data_cleaned = title_data_cleaned.drop('time', axis=1)\n",
    "    \n",
    "    print(title_data_cleaned.head(5))\n",
    "    print()\n",
    "    \n",
    "    # Export data to csv\n",
    "    title_data_cleaned.to_csv(title_cwd + '/' + csv[:-4] + '_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adult-yeast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Title  timestamp\n",
      "0                                1st thing bring lube 2020-05-02\n",
      "1         the bond market crash begins on mayth andth 2020-05-02\n",
      "2                    make insider trading legal again 2020-05-02\n",
      "3                                             retreat 2020-05-02\n",
      "4                                      u bryguysayshi 2020-05-02\n",
      "5   i should of held ended up selling all for brea... 2020-05-02\n",
      "6   this documentary about autism made me think of... 2020-05-02\n",
      "7   why is there such price difference on internat... 2020-05-02\n",
      "8   just thought this should be reposted based of ... 2020-05-02\n",
      "9                               better to spxu or put 2020-05-02\n",
      "10          maybe elon will save me whatever it takes 2020-05-02\n",
      "11                                     this aged well 2020-05-02\n",
      "12                            attention all wsb users 2020-05-02\n",
      "13                        the market isnt the economy 2020-05-02\n",
      "14           question about setting limits on options 2020-05-02\n",
      "15                              robinhood custom list 2020-05-02\n",
      "16  thank you elon lost money but you gave me redd... 2020-05-02\n",
      "17                      kim jong un is alive and well 2020-05-02\n",
      "18                                               kekw 2020-05-02\n"
     ]
    }
   ],
   "source": [
    "print(title_data_cleaned[:19])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-feeling",
   "metadata": {},
   "source": [
    "# Clean Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_cwd = cwd + '/comment'\n",
    "_, _, comment_names = next(os.walk(comment_cwd))\n",
    "print(comment_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_names = [file for file in comment_names if file[-4:] == '.csv']\n",
    "print(comment_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows preview of each csv\n",
    "for csv in comment_names:\n",
    "    csv_comment = pd.read_csv(comment_cwd + '/' + csv)\n",
    "    csv_comment = csv_comment.dropna()\n",
    "    print(csv_comment.head(5))\n",
    "    print(csv_comment.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows preview of each csv\n",
    "for csv in comment_names:\n",
    "    csv_comment = pd.read_csv(comment_cwd + '/' + csv)\n",
    "    csv_comment = csv_comment.dropna()\n",
    "    csv_comment.Comment = csv_comment.Comment.str.lower()\n",
    "    \n",
    "    comment_data_cleaned = csv_comment.copy()\n",
    "    \n",
    "#     print(comment_data_cleaned)\n",
    "    \n",
    "    comment_data_cleaned.Comment = comment_data_cleaned.Comment.apply(lambda x:re.sub('@[^\\s]+','',x))\n",
    "    # Remove URLS\n",
    "    comment_data_cleaned.Comment = comment_data_cleaned.Comment.apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n",
    "    # Remove all the special characters\n",
    "    comment_data_cleaned.Comment = comment_data_cleaned.Comment.apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n",
    "    # Remove numbers\n",
    "    comment_data_cleaned.Comment = comment_data_cleaned.Comment.apply(lambda x:re.sub(' \\d+', '', x))\n",
    "    # Remove all single characters\n",
    "    comment_data_cleaned.Comment = comment_data_cleaned.Comment.apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x))\n",
    "    # Remove single characters again because still leaving some\n",
    "    comment_data_cleaned.Comment = comment_data_cleaned.Comment.apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x))\n",
    "    # Removes specific time of day from timestamp\n",
    "#     comment_data_cleaned.timestamp = pd.to_datetime(csv_comment.timestamp).dt.date\n",
    "    \n",
    "    comment_data_cleaned = comment_data_cleaned[comment_data_cleaned.Comment != 'deleted']\n",
    "    comment_data_cleaned = comment_data_cleaned[comment_data_cleaned.Comment != 'removed']\n",
    "    csv_comment = csv_comment.dropna()\n",
    "    \n",
    "    # CUTOFFS: 0:00, 9:00, 13:00, 17:00\n",
    "\n",
    "    comment_data_cleaned.timestamp = pd.to_datetime(comment_data_cleaned.timestamp).dt.floor('H')\n",
    "#     print(title_data_cleaned.timestamp)\n",
    "    comment_data_cleaned.loc[pd.to_datetime(comment_data_cleaned.timestamp).dt.hour < 9, 'time'] = datetime.time(0)\n",
    "    comment_data_cleaned.loc[pd.to_datetime(comment_data_cleaned.timestamp).dt.hour >= 9, 'time'] = datetime.time(9)\n",
    "    comment_data_cleaned.loc[pd.to_datetime(comment_data_cleaned.timestamp).dt.hour >= 13, 'time'] = datetime.time(13)\n",
    "    comment_data_cleaned.loc[pd.to_datetime(comment_data_cleaned.timestamp).dt.hour >= 17, 'time'] = datetime.time(17)\n",
    "#     print(type(comment_data_cleaned.timestamp)comment_data_cleaned)\n",
    "\n",
    "    comment_data_cleaned.timestamp = pd.to_datetime(comment_data_cleaned.timestamp).dt.date\n",
    "    \n",
    "    comment_data_cleaned.timestamp = pd.to_datetime(comment_data_cleaned.timestamp).dt.strftime('%Y-%m-%d')\n",
    "    comment_data_cleaned.time = comment_data_cleaned.time.astype(str)\n",
    "    \n",
    "    \n",
    "    comment_data_cleaned.timestamp = pd.to_datetime(comment_data_cleaned.timestamp + ' ' + comment_data_cleaned.time)\n",
    "#     comment_data_cleaned = comment_data_cleaned.drop('time', axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Export data to csv\n",
    "    comment_data_cleaned.to_csv(comment_cwd + '/' + csv[:-4] + '_cleaned.csv')\n",
    "    print(comment_data_cleaned.head(5))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-youth",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
